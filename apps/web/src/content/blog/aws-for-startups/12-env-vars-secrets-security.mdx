---
title: "Secrets & Agent Security: Drawing the Line Between Helpful and Dangerous"
description: "Secure your environment variables, integrate secrets management, and establish agent execution security. Four sandboxing levels from scoped IAM to VM isolation."
excerpt: "Drawing the line between helpful and dangerous. dotenv patterns, secrets management, and the four levels of agent execution security."
date: "2026-02-20"
author: "works-on-my.cloud"
tags: ["aws", "devops", "startup", "security", "secrets-manager"]
series: "AWS From Zero to Production"
seriesPart: 12
featured: false
draft: true
---

import Alert from '../../../components/shared/Alert.astro';
import ValidationChecklist from '../../../components/blog/guide/ValidationChecklist.astro';
import FileTree from '../../../components/blog/code/FileTree.astro';
import Command from '../../../components/blog/code/Command.astro';
import TerminalOutput from '../../../components/blog/code/TerminalOutput.astro';
import EnvVars from '../../../components/blog/guide/EnvVars.astro';
import EnvVar from '../../../components/blog/guide/EnvVar.astro';
import ComparisonTable from '../../../components/blog/guide/ComparisonTable.astro';
import ComparisonHeader from '../../../components/blog/guide/ComparisonHeader.astro';
import ComparisonRow from '../../../components/blog/guide/ComparisonRow.astro';

Your agent needs database credentials to generate a migration script. You paste the connection string into the chat window. The agent works perfectly. It generates clean, valid SQL. Three days later you are reviewing the agent's output and notice it wrote the connection string into a comment in the migration file: `-- Connection: postgresql://admin:S3cretP@ss@prod-db.us-east-1.rds.amazonaws.com:5432/myapp`. That comment is now in your git history. Your production database credentials are in every clone of your repository.

**Time:** About 45 minutes.

**Outcome:** Secure secrets management with AWS Secrets Manager, dotenv patterns that prevent leaks, and four levels of agent sandboxing from minimum viable to production-grade.

---

## Why This Matters

Agents process everything in their context. This includes your code, your error messages, your configuration files, and any credentials you share. The agent does not distinguish between "information you are showing me for context" and "information I can reference freely in my output." To the agent, a database password pasted into the chat is the same as a variable name pasted into the chat: known information that can appear anywhere in the response.

This creates a credential exposure surface area that multiplies with agent usage. Every credential you share with an agent is a credential that can appear in code comments, log statements, configuration files, error messages, commit messages, and generated documentation. The agent is not being malicious. It is being thorough. It references everything in context because that is how it produces useful output.

The attack surface is not the agent itself. The attack surface is the output. Agent output flows into files, commits, CI logs, PR descriptions, and Slack summaries. Each of those destinations has different access controls, retention policies, and visibility. A credential that enters the agent's context can end up in any of them.

Three facts drive this entire post:

1. **Credentials in agent context will eventually leak into output.** Not every time. Not predictably. But the more credentials an agent processes, the higher the probability that one appears somewhere you did not intend. The only way to reduce this probability to zero is to never put real credentials in the agent's context.

2. **Environment variables injected by the shell are invisible to the agent's context window.** When you use `aws-vault exec` or `op run` to inject credentials as environment variables, the agent can use them (through SDK calls and CLI commands) but cannot read them as text. This is the key insight that makes secure agent credential management possible.

3. **Sandboxing is defense in depth.** Credential scoping prevents the agent from accessing secrets it should not have. Sandboxing prevents the agent from doing damage even with the credentials it does have. You need both.

---

## What We're Building

- A `.env` / `.env.example` pattern that prevents credential leaks to git
- AWS Secrets Manager integration for storing and retrieving secrets programmatically
- 1Password CLI integration as a local development alternative
- Agent credential scoping with `aws-vault` for safe AWS access
- Four sandboxing levels for agent execution security
- AGENT-INSTRUCTIONS.md additions: Secrets & Credentials (5 lines) + Agent Execution Security (6 lines), cumulative 43 lines

---

## dotenv Patterns

The foundation of credential management is simple: `.env` files hold real values and are gitignored. `.env.example` files hold placeholders and are committed. Every developer (and every agent) copies `.env.example` to `.env` and fills in real values locally.

This pattern is decades old. It is also the pattern agents violate most frequently, because agents in their eagerness to produce working code will hardcode values they find in their context rather than referencing environment variables.

### The .env.example File

Your `.env.example` is the contract. It documents every environment variable the project needs, with placeholder values that make the expected format obvious:

<EnvVars>
  <EnvVar
    name="DATABASE_URL"
    required
    description="PostgreSQL connection string. Format: postgresql://user:password@host:port/database"
    default="postgresql://postgres:postgres@localhost:5432/myapp_dev"
  />
  <EnvVar
    name="REDIS_URL"
    required
    description="Redis connection string for caching and sessions."
    default="redis://localhost:6379"
  />
  <EnvVar
    name="AWS_PROFILE"
    required
    description="Named AWS profile for CLI and SDK operations. Never use access keys directly."
    default="myapp-dev"
  />
  <EnvVar
    name="AWS_DEFAULT_REGION"
    required
    description="AWS region for all resource operations."
    default="ap-south-1"
  />
  <EnvVar
    name="APP_SECRET"
    required
    description="Application secret for session signing. Generate with: openssl rand -hex 32"
  />
  <EnvVar
    name="STRIPE_SECRET_KEY"
    description="Stripe API key. Use test mode key (sk_test_...) for local development."
  />
  <EnvVar
    name="SENDGRID_API_KEY"
    description="SendGrid API key for transactional email. Optional for local dev."
  />
</EnvVars>

The default values for `DATABASE_URL` and `REDIS_URL` match the Docker Compose services from [Part 10](/blog/aws-for-startups/10-docker-compose-local). New developers run `docker compose up`, copy `.env.example` to `.env`, and everything connects without editing a single value.

### File Placement and Gitignore

<FileTree>
my-startup/
  .env
  .env.example
  .env.test
  .gitignore
  docker-compose.yml
  src/
    config.ts
</FileTree>

Your `.gitignore` must include every `.env` variant except `.env.example`:

```text title=".gitignore"
# Environment variables (NEVER commit real values)
.env
.env.local
.env.*.local
.env.production
.env.staging

# Keep .env.example and .env.test committed
!.env.example
!.env.test
```

The `.env.test` file is committed with test-safe values (local database, mock API keys). It is used by your test runner so CI does not need real credentials.

### Loading Environment Variables

```typescript title="src/config.ts"
import { z } from 'zod';

const envSchema = z.object({
  DATABASE_URL: z.string().url(),
  REDIS_URL: z.string().url(),
  AWS_PROFILE: z.string().min(1),
  AWS_DEFAULT_REGION: z.string().min(1),
  APP_SECRET: z.string().min(32),
  STRIPE_SECRET_KEY: z.string().startsWith('sk_').optional(),
  SENDGRID_API_KEY: z.string().optional(),
});

export const config = envSchema.parse(process.env);
```

Zod validation at startup catches missing or malformed environment variables immediately. If someone copies `.env.example` but forgets to fill in `APP_SECRET`, the application fails with a clear error message instead of silently using an empty string.

---

## AWS Secrets Manager Integration

`.env` files work for local development. For deployed environments (staging, production), you need secrets stored outside your codebase entirely. AWS Secrets Manager stores, encrypts, and rotates secrets with fine-grained IAM access control.

### Creating a Secret

```bash terminal
aws secretsmanager create-secret \
  --name "myapp/production/database" \
  --description "Production PostgreSQL connection string" \
  --secret-string '{"url":"postgresql://app:REAL_PASSWORD@prod-db.us-east-1.rds.amazonaws.com:5432/myapp"}' \
  --tags Key=Environment,Value=production Key=Project,Value=myapp Key=ManagedBy,Value=manual
```

Use a path-based naming convention: `{project}/{environment}/{secret-name}`. This maps cleanly to IAM policies. You can grant an agent access to `myapp/dev/*` without exposing `myapp/production/*`.

### Retrieving Secrets in Code

:::code-switcher
```typescript title="secrets.ts (Bun/Node.js)"
import {
  SecretsManagerClient,
  GetSecretValueCommand,
} from '@aws-sdk/client-secrets-manager';

const client = new SecretsManagerClient({ region: 'ap-south-1' });

export async function getSecret(secretName: string): Promise<Record<string, string>> {
  const response = await client.send(
    new GetSecretValueCommand({ SecretId: secretName })
  );

  if (!response.SecretString) {
    throw new Error(`Secret ${secretName} has no string value`);
  }

  return JSON.parse(response.SecretString);
}

// Usage
const dbSecret = await getSecret('myapp/production/database');
const databaseUrl = dbSecret.url;
```

```python title="secrets.py"
import json
import boto3

def get_secret(secret_name: str) -> dict:
    client = boto3.client("secretsmanager", region_name="ap-south-1")
    response = client.get_secret_value(SecretId=secret_name)
    return json.loads(response["SecretString"])

# Usage
db_secret = get_secret("myapp/production/database")
database_url = db_secret["url"]
```

```go title="secrets.go"
package config

import (
	"context"
	"encoding/json"

	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/secretsmanager"
)

func GetSecret(ctx context.Context, secretName string) (map[string]string, error) {
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion("ap-south-1"))
	if err != nil {
		return nil, err
	}

	client := secretsmanager.NewFromConfig(cfg)
	output, err := client.GetSecretValue(ctx, &secretsmanager.GetSecretValueInput{
		SecretId: &secretName,
	})
	if err != nil {
		return nil, err
	}

	var result map[string]string
	err = json.Unmarshal([]byte(*output.SecretString), &result)
	return result, err
}
```
:::

### Automatic Rotation

Secrets Manager can rotate secrets automatically on a schedule. For database credentials, AWS provides Lambda rotation functions that update both the secret and the database password:

```bash terminal
aws secretsmanager rotate-secret \
  --secret-id "myapp/production/database" \
  --rotation-lambda-arn arn:aws:lambda:ap-south-1:123456789012:function:SecretsManagerRotation \
  --rotation-rules '{"AutomaticallyAfterDays": 30}'
```

You do not need rotation on day one. Set it up when you have a production database (we will revisit this in [Part 37](/blog/aws-for-startups/37-secrets-manager) with a full Secrets Manager deep dive). For now, know that the capability exists and that it is one of the reasons to use Secrets Manager over a `.env` file on an EC2 instance.

### Cost

Secrets Manager pricing is straightforward: **$0.40 per secret per month** plus **$0.05 per 10,000 API calls**. For a startup with 10 secrets and moderate retrieval, that is about $4-5/month. Cache secret values in memory (with a TTL of 5-10 minutes) to keep API call costs negligible.

:::tip
The Docker Compose environment from [Part 10](/blog/aws-for-startups/10-docker-compose-local) uses `.env` files for local secrets. Deployed environments use Secrets Manager. Your `config.ts` should check for an environment flag (`IS_LOCAL=true`) and load from the appropriate source. This keeps local development fast while production stays secure.
:::

---

## 1Password CLI Integration

If your team uses 1Password, the CLI provides an excellent alternative for local development. Instead of maintaining `.env` files at all, you inject secrets directly from your vault at runtime.

### Setup

```bash terminal
brew install --cask 1password-cli
```

Verify the installation and authenticate:

<Command cmd="op account list" description="Confirm 1Password CLI is configured" />

### Injecting Secrets with op run

Create a template file that references 1Password items:

```text title=".env.op"
DATABASE_URL=op://Development/myapp-db/connection-string
REDIS_URL=op://Development/myapp-redis/url
APP_SECRET=op://Development/myapp-secrets/app-secret
STRIPE_SECRET_KEY=op://Development/stripe-test/secret-key
```

Run your application with secrets injected:

```bash terminal
op run --env-file=.env.op -- bun run dev
```

The `op run` command resolves every `op://` reference, injects the real values as environment variables, and runs your command. The real values never touch your filesystem. There is no `.env` file to accidentally commit, no plaintext secrets sitting on disk, and no risk of an agent reading credentials from a file.

### When to Use 1Password vs Secrets Manager

| Scenario | Use |
|----------|-----|
| Local development | 1Password CLI (`op run`) |
| CI/CD pipelines | AWS Secrets Manager or GitHub Secrets |
| Deployed applications (staging/prod) | AWS Secrets Manager |
| Agent sessions | `aws-vault exec` (see next section) |
| Sharing secrets across team | 1Password shared vault |

1Password is for humans on their local machines. Secrets Manager is for code running in AWS. The boundary is your laptop vs the cloud.

---

## Agent Credential Scoping

This is the most important section in this post. Everything else is standard credential hygiene. This section is specific to working with AI coding agents.

**Rule: NEVER pass production credentials to agent sessions.** Not in the chat. Not in a file the agent can read. Not as an environment variable visible in the agent's context. Never.

The reason is not that the agent is untrustworthy. The reason is that agent output is unpredictable. You cannot guarantee what the agent will include in its response. A database password pasted into the chat might appear in a generated config file, a code comment, a log statement, or a PR description. The agent does not understand that some strings are secrets. It treats all context equally.

### The Dangerous Setup

```bash terminal
# DO NOT DO THIS
export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
# Now start your agent session with these credentials in the environment
```

This gives the agent your personal AWS credentials. The agent can call any AWS API your user has access to. If your IAM user has AdministratorAccess (common for startup founders), the agent can create resources, delete resources, access any secret, and modify IAM policies. Worse, those credential values might appear in the agent's output.

### The Safe Setup

```bash terminal
aws-vault exec myapp-dev -- claude
```

`aws-vault` does three things:

1. **Assumes a role** with a scoped IAM policy (the `myapp-dev` profile you configured in [Part 2](/blog/aws-for-startups/02-iam-intro-for-starters))
2. **Generates temporary credentials** (valid for 1 hour by default, configurable)
3. **Injects them as environment variables** that the agent session inherits

The agent can use these credentials through the AWS SDK and CLI, but the credentials are environment variables, not text in the agent's context window. The agent cannot read them, paste them, or write them to a file. It can only use them to make authenticated API calls.

### Scoping Agent IAM Roles

Create a dedicated IAM role for agent sessions:

```json title="agent-dev-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowDevResourceManagement",
      "Effect": "Allow",
      "Action": [
        "s3:*",
        "dynamodb:*",
        "lambda:*",
        "logs:*",
        "cloudformation:*"
      ],
      "Resource": "arn:aws:*:ap-south-1:123456789012:*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/Environment": "dev"
        }
      }
    },
    {
      "Sid": "DenyProductionAccess",
      "Effect": "Deny",
      "Action": "*",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/Environment": "production"
        }
      }
    }
  ]
}
```

The explicit `Deny` on production-tagged resources is a hard boundary. Even if the `Allow` statement is broader than intended, the `Deny` prevents any operation on production resources. IAM `Deny` always wins over `Allow`.

For production queries (verifying a migration, checking a deployment), create a separate read-only role:

```json title="agent-prod-readonly-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ReadOnlyProduction",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket",
        "dynamodb:GetItem",
        "dynamodb:Query",
        "dynamodb:Scan",
        "rds:Describe*",
        "logs:GetLogEvents",
        "logs:FilterLogEvents",
        "cloudwatch:GetMetricData"
      ],
      "Resource": "*"
    },
    {
      "Sid": "DenyAllWrites",
      "Effect": "Deny",
      "Action": [
        "s3:PutObject",
        "s3:DeleteObject",
        "dynamodb:PutItem",
        "dynamodb:UpdateItem",
        "dynamodb:DeleteItem",
        "rds:Modify*",
        "rds:Delete*"
      ],
      "Resource": "*"
    }
  ]
}
```

<Alert type="caution" title="Agent Trap">

An agent requests production database access "to verify the migration works." This sounds reasonable. The agent is being thorough, trying to confirm that the migration it generated actually runs cleanly against real data.

But an agent with production database credentials can run any query, including destructive ones. The agent's training data includes examples of running migrations directly against production databases. Your `AGENT-INSTRUCTIONS.md` rule is explicit: "NEVER pass production credentials to agent sessions."

**The fix:** Give agents a read-only replica connection for verification. Use `aws-vault exec myapp-prod-readonly` for the session. The agent can run `SELECT` queries and `DESCRIBE` statements but cannot execute `DROP TABLE`, `DELETE FROM`, or `ALTER TABLE`. Never the primary.

</Alert>

---

## Agent Execution Security: Four Sandboxing Levels

Credential scoping controls what the agent can access. Sandboxing controls what the agent can do on your machine. You need both.

<ComparisonTable>
  <ComparisonHeader columns={["Level 1", "Level 2", "Level 3", "Level 4"]} />
  <ComparisonRow feature="Name" Level_1="Scoped IAM" Level_2="Directory + IAM" Level_3="Container + IAM" Level_4="VM + IAM" />
  <ComparisonRow feature="AWS access" Level_1="Scoped role" Level_2="Scoped role" Level_3="Scoped role" Level_4="Scoped role" />
  <ComparisonRow feature="File access" Level_1="Entire filesystem" Level_2="Repository only" Level_3="Container volume" Level_4="VM filesystem" />
  <ComparisonRow feature="Network access" Level_1="Unrestricted" Level_2="Unrestricted" Level_3="Restricted (Best)" Level_4="Restricted (Best)" />
  <ComparisonRow feature="Process isolation" Level_1="None" Level_2="None" Level_3="Container namespace" Level_4="Full VM (Best)" />
  <ComparisonRow feature="Setup effort" Level_1="5 minutes (Best)" Level_2="15 minutes" Level_3="30 minutes" Level_4="60+ minutes" />
  <ComparisonRow feature="Use case" Level_1="Quick exploration" Level_2="Daily development" Level_3="CI agent runs" Level_4="Production operations" />
</ComparisonTable>

### Level 1: Scoped IAM Only

This is what you have had since [Part 2](/blog/aws-for-startups/02-iam-intro-for-starters). The agent runs with a scoped IAM role via `aws-vault exec`, but has full access to your filesystem and network. It is the minimum viable security posture. Fine for quick questions and exploration, not for sustained agent sessions.

### Level 2: Directory Restriction + Scoped IAM

The minimum for this curriculum. The agent session can only read and write files within your repository directory. Most agent CLI tools support this natively. Claude Code runs in the directory you invoke it from. Cursor and similar editors are scoped to the project directory by default.

Enforce it explicitly:

```bash terminal
aws-vault exec myapp-dev -- claude --allowedTools "Edit,Write,Read,Bash" --directory ~/projects/myapp
```

The `--directory` flag (or equivalent) restricts filesystem access. The agent cannot read your SSH keys, your AWS credentials file, or other project directories. Combined with scoped IAM, the agent can only modify your project files and call dev-environment AWS APIs.

### Level 3: Container Execution + Scoped IAM

Recommended for CI and automated agent runs. The agent runs inside a Docker container with only the repository mounted as a volume. Network access is restricted to AWS APIs and package registries.

```yaml title="docker-compose.agent.yml"
services:
  agent-session:
    image: node:22-slim
    working_dir: /workspace
    volumes:
      - .:/workspace
    environment:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_SESSION_TOKEN
      - AWS_DEFAULT_REGION=ap-south-1
    networks:
      - agent-network

networks:
  agent-network:
    driver: bridge
    internal: false
```

The container inherits temporary AWS credentials from the host (via `aws-vault exec`), but the agent cannot access anything outside the mounted volume. No SSH keys, no browser cookies, no other project directories. Network egress rules can further restrict which endpoints the container can reach.

### Level 4: VM Execution + Scoped IAM

For production-level agent operations (agents making changes to staging or production infrastructure). The agent runs inside a dedicated virtual machine (EC2, Fly.io, or a local VM via Lima/Colima). The VM has its own IAM role attached via the instance profile, separate from your personal credentials.

This is overkill for a startup in the first 12 parts of this series. You will not need Level 4 until agents are running infrastructure changes in CI pipelines, which starts in [Part 27](/blog/aws-for-startups/27-testing-in-ci). Mention it here so the progression makes sense when you get there.

### Decision Guide

| Situation | Minimum Level |
|-----------|---------------|
| Asking an agent a question about your code | Level 1 |
| Agent generating code you will review | Level 2 |
| Agent running in CI/CD pipeline | Level 3 |
| Agent modifying staging/production infrastructure | Level 4 |
| Agent accessing production data (read-only) | Level 3 |

When in doubt, go one level higher. The setup cost is measured in minutes. The cost of an agent with too much access is measured in incidents.

---

## AGENT-INSTRUCTIONS.md Additions

Add two new sections to your `AGENT-INSTRUCTIONS.md`. These bring the file from 32 lines (after [Part 9](/blog/aws-for-startups/09-monorepo-context-evals)) to 43 lines.

### Secrets & Credentials Section (5 lines)

```markdown title="AGENT-INSTRUCTIONS.md (append)"
## Secrets & Credentials
- NEVER hardcode credentials, API keys, or secrets in source code
- Use AWS Secrets Manager for all sensitive values in deployed environments
- .env files are gitignored; .env.example committed with placeholder values
- NEVER pass production credentials to agent sessions
- Use aws-vault exec for scoped, temporary credential injection
```

### Agent Execution Security Section (6 lines)

```markdown title="AGENT-INSTRUCTIONS.md (append)"
## Agent Execution Security
- Agent CLI sessions run in isolated environments (Level 2 minimum)
- Agent network access: allow AWS APIs and package registries only
- Agent file access: scoped to repository directory
- Use aws-vault exec or IAM roles for agent credential injection
- Agent sessions have max duration (30 min inactivity timeout)
- All agent-executed commands logged to audit trail
```

The secrets rules are non-negotiable. The execution security rules set the baseline. Both are enforceable: gitleaks catches hardcoded credentials in [Part 8](/blog/aws-for-startups/08-pre-commit-code-quality), and the sandboxing levels are configuration, not convention.

---

## The Fine Line

<Alert type="warning" title="The Fine Line">

| | |
|---|---|
| ‚ùå **Under** | Hardcoded credentials in source code. `.env` committed to git. Agents with your personal AWS credentials. You find out credentials leaked when AWS emails you about unusual API activity. |
| ‚úÖ **Right** | AWS Secrets Manager for deployed environments. `.env.example` pattern for local dev. aws-vault for scoped agent credentials. Level 2 sandboxing (directory restriction). gitleaks catching credential patterns in pre-commit. |
| ‚ùå **Over** | HashiCorp Vault cluster with auto-unseal, dynamic database credentials, and PKI infrastructure for mutual TLS between services. You have one database and two developers. |
| ü§ñ **Agent Trap** | Agent writes credentials to config files, log statements, or code comments. It does this because the credential was in its context, and it treats everything in context as "known information" that can be referenced anywhere. gitleaks catches the pattern on commit, but the damage is in the agent's output history. The real fix is never putting real credentials in agent context. Use aws-vault exec to inject credentials as environment variables the agent cannot read from its context. |

</Alert>

---

## What's Coming

Next in **Part 13: S3 Static Hosting**, we deploy your first resource to AWS. The secrets management you set up today protects the credentials Terraform needs. The Docker Compose environment you built in [Part 10](/blog/aws-for-startups/10-docker-compose-local) is your testing ground. Everything connects.

---

## Validation Checklist

<ValidationChecklist items={[
  {
    category: "Secrets Management",
    tasks: [
      { text: ".env added to .gitignore and not tracked in git", syncKey: "part-12-env-gitignored" },
      { text: ".env.example committed with placeholder values for all required variables", syncKey: "part-12-env-example" },
      { text: "AWS Secrets Manager configured with at least one secret (aws secretsmanager list-secrets)", syncKey: "part-12-secrets-manager" },
      { text: "No hardcoded credentials in codebase (gitleaks detects 0 findings)", syncKey: "part-12-no-hardcoded" }
    ]
  },
  {
    category: "Agent Security",
    tasks: [
      { text: "Agent sandboxing at Level 2 minimum (directory-scoped sessions)", syncKey: "part-12-sandboxing" },
      { text: "aws-vault configured with scoped dev profile for agent sessions", syncKey: "part-12-aws-vault" },
      { text: "Agent sessions cannot access production credentials directly (Deny policy in place)", syncKey: "part-12-no-prod-creds" }
    ]
  },
  {
    category: "AGENT-INSTRUCTIONS.md",
    tasks: [
      { text: "Secrets & Credentials section added (5 rules)", syncKey: "part-12-instructions-secrets" },
      { text: "Agent Execution Security section added (6 rules)", syncKey: "part-12-instructions-security" },
      { text: "Cumulative file at 43 lines", syncKey: "part-12-instructions-total" }
    ]
  }
]} />

---

## Key Takeaways

1. Agents process everything in their context, so never paste production credentials into an agent session, because those credentials will appear in output you cannot fully control.
2. `aws-vault exec` is the safest way to give agents AWS access: scoped, temporary, and the credentials exist as environment variables, not text in the agent's context.
3. Agent sandboxing has four levels. Level 2 (directory restriction + scoped IAM) is the minimum for this curriculum. Level 3 (container) is recommended when agents run in CI.
4. `.env` files are gitignored. `.env.example` files are committed with placeholders. This is non-negotiable when agents can read your filesystem.

---

*Have questions? Found an error? [Open an issue](https://github.com/hionnode/akasha-lekha/issues) or reach out on [LinkedIn](https://linkedin.com/in/chinmay-pandey).*
