---
title: "Production Deployment: The Pipeline With a Human Gate"
description: "Agent-assisted production deployment with non-negotiable human approval. Staging validation, deployment summary, monitoring, and Recalibration Checkpoint 3."
excerpt: "The pipeline with a human gate. Agent-assisted deployment to production: staging validation, explain summary, and a non-negotiable human approval step."
date: "2026-07-06"
author: "works-on-my.cloud"
tags: ["aws", "devops", "startup", "ci-cd", "ai-agents"]
series: "aws-for-startups"
seriesPart: 47
draft: true
---

import Alert from '../../../components/shared/Alert.astro';
import ValidationChecklist from '../../../components/blog/guide/ValidationChecklist.astro';
import FileTree from '../../../components/blog/code/FileTree.astro';
import TerminalOutput from '../../../components/blog/code/TerminalOutput.astro';
import ComparisonTable from '../../../components/blog/guide/ComparisonTable.astro';
import ComparisonHeader from '../../../components/blog/guide/ComparisonHeader.astro';
import ComparisonRow from '../../../components/blog/guide/ComparisonRow.astro';

Your agent opens a PR. CI passes. Tests green. Security scans clean. Terraform plan shows 2 to change, 0 to destroy. The agent adds a helpful comment: "Ready for production deployment." You glance at the summary, approve, and the pipeline ships it. Twelve minutes later, your health check starts failing. The agent updated the ECS task definition with a new environment variable that references a Secrets Manager path that does not exist in production. CI cannot catch missing runtime configuration. The agent did not know the difference between staging and production secrets. But you approved it without reading the deployment summary.

**Time:** About 60 minutes.

**Outcome:** A production deployment pipeline with staging validation, an automated explain summary, a non-negotiable human approval gate, post-deploy monitoring for 15 minutes, and a clear rollback strategy. Plus four new Scorecard panels and Recalibration Checkpoint 3 for your agent workflow.

---

## Why This Matters

Everything in this series has been building toward this moment. You have containers running on ECS from [Part 41](/blog/aws-for-startups/41-ecs-fargate-bun). You have CI that catches code quality issues from [Part 45](/blog/aws-for-startups/45-github-actions-ci). You have OIDC for secure AWS access from [Part 46](/blog/aws-for-startups/46-github-actions-oidc). Now you need the pipeline that moves code from "merged to main" to "running in production."

This is where trust gets tested. Not the abstract concept of trust from the style guide. Concrete trust, measured in data.

Every deployment is a bet. You are betting that the code changes are correct, that the infrastructure changes are safe, that the runtime configuration is complete, and that nothing in the interaction between new code and existing state will cause a failure. CI reduces the odds of a bad bet. Review reduces it further. But the odds are never zero.

The deployment pipeline's job is not to eliminate risk. It is to do three things:

1. **Surface all available information before the decision.** What changed? What was verified? What is the blast radius? What are the known risks?
2. **Force a conscious decision.** Not a rubber stamp. Not an auto-approve. A human reviews the deployment summary and clicks "approve" with full context.
3. **Limit the damage window.** If something goes wrong, detect it fast and make rollback cheap.

---

## What We're Building

- A production deployment workflow triggered by merge to `main`
- Staging deployment with automated smoke tests and a quick K6 load test
- An explain summary generated by the pipeline: changes, verifications, risks
- GitHub Environment with required reviewers as the human approval gate
- Post-deploy monitoring for 15 minutes with automated anomaly alerts
- Rollback strategy with ECS service rollback and Terraform state management
- Four new Scorecard panels: PR-to-production lead time, deployment frequency, rollback rate, post-deploy error rate

---

## The Deployment Flow

The full flow from merge to verified production:

<Alert type="important" title="Production Deployment Flow">

```
  Merge to main
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build &     â”‚  Docker images tagged with git SHA
â”‚  Push to ECR â”‚  Pushed via OIDC production role
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deploy to   â”‚  ECS task definition updated
â”‚  Staging     â”‚  New tasks rolled out
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Staging     â”‚  Smoke tests + K6 quick load
â”‚  Validation  â”‚  Compare response times to baseline
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Explain     â”‚  What changed, what was verified,
â”‚  Summary     â”‚  risk assessment, posted to PR
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HUMAN       â”‚  GitHub Environment approval
â”‚  APPROVAL    â”‚  Required reviewer must approve
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deploy to   â”‚  ECS task definition updated
â”‚  Production  â”‚  Rolling deployment
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitor     â”‚  15 minutes of health checks
â”‚  (15 min)    â”‚  Error rate, latency, CPU/memory
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€ All clear â”€â”€â–º Done
       â”‚
       â””â”€â”€ Anomaly â”€â”€â–º Alert + rollback decision (human)
```

</Alert>

Seven steps. Five are automated. Two require human judgment: the approval gate and the rollback decision. That split is intentional.

---

## Build and Push to ECR

After merge to `main`, the first job builds Docker images and pushes them to ECR. This uses the production OIDC role from [Part 46](/blog/aws-for-startups/46-github-actions-oidc).

```yaml title=".github/workflows/deploy-production.yml"
name: Production Deploy

on:
  push:
    branches: [main]

concurrency:
  group: production-deploy
  cancel-in-progress: false

permissions:
  contents: read
  id-token: write
  pull-requests: write

env:
  AWS_REGION: us-east-1
  PRODUCTION_ROLE_ARN: arn:aws:iam::123456789012:role/shipfast-github-production
  ECR_REGISTRY: 123456789012.dkr.ecr.us-east-1.amazonaws.com

jobs:
  build-push:
    name: Build & Push
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ github.sha }}

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.PRODUCTION_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push API
        uses: docker/build-push-action@v6
        with:
          context: services/api-bun
          file: services/api-bun/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/shipfast-api:${{ github.sha }}
            ${{ env.ECR_REGISTRY }}/shipfast-api:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
```

`cancel-in-progress: false` on the concurrency group is different from CI. In CI, canceling stale runs saves time. In production deployments, canceling a running deployment can leave your service in an inconsistent state. Let the current deployment finish, then start the next one.

The image gets two tags: the git SHA (immutable, traceable) and `latest` (convenience for local development). Production always deploys by SHA, never by `latest`.

---

## Staging Deployment and Validation

Before production, deploy to staging and run automated validation. This catches runtime issues that CI cannot: missing environment variables, database migration mismatches, network connectivity problems.

```yaml title=".github/workflows/deploy-production.yml"
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build-push]

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.PRODUCTION_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy to staging ECS
        run: |
          aws ecs update-service \
            --cluster shipfast-staging \
            --service shipfast-api \
            --force-new-deployment \
            --task-definition $(
              aws ecs register-task-definition \
                --cli-input-json file://ecs/task-definition.json \
                --query 'taskDefinition.taskDefinitionArn' \
                --output text
            )

      - name: Wait for staging deployment
        run: |
          aws ecs wait services-stable \
            --cluster shipfast-staging \
            --services shipfast-api
          echo "Staging deployment stable"

  validate-staging:
    name: Staging Validation
    runs-on: ubuntu-latest
    needs: [deploy-staging]

    steps:
      - uses: actions/checkout@v4

      - name: Smoke tests
        run: |
          STAGING_URL="https://staging.shipfast.dev"

          # Health check
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$STAGING_URL/health")
          if [ "$STATUS" != "200" ]; then
            echo "Health check failed: HTTP $STATUS"
            exit 1
          fi
          echo "Health check passed"

          # API smoke tests
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$STAGING_URL/api/v1/status")
          if [ "$STATUS" != "200" ]; then
            echo "API status check failed: HTTP $STATUS"
            exit 1
          fi
          echo "API smoke tests passed"

      - name: K6 quick load test
        uses: grafana/k6-action@v0.3.1
        with:
          filename: tests/load/smoke.js
        env:
          K6_BASE_URL: https://staging.shipfast.dev
          K6_THRESHOLDS_HTTP_REQ_DURATION: "p(95)<500"
          K6_THRESHOLDS_HTTP_REQ_FAILED: "rate<0.01"

      - name: Compare with baseline
        run: |
          echo "Comparing staging metrics with production baseline..."
          # Fetch production p95 latency from CloudWatch
          PROD_P95=$(aws cloudwatch get-metric-statistics \
            --namespace "ECS/Shipfast" \
            --metric-name "ResponseTime" \
            --dimensions Name=Environment,Value=production \
            --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
            --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
            --period 3600 \
            --statistics p95 \
            --query 'Datapoints[0].p95' \
            --output text)

          echo "Production p95: ${PROD_P95}ms"
          echo "Staging p95 from K6: check k6 output above"
          echo "If staging p95 > 2x production p95, investigate before approving"
```

The K6 smoke test is not a full load test. It sends 10-50 requests over 30 seconds with two thresholds: p95 latency under 500ms and error rate under 1%. The purpose is regression detection, not capacity planning. You already have a full load test from [Part 44](/blog/aws-for-startups/44-k6-containers).

The baseline comparison fetches production's current p95 latency from CloudWatch and prints it alongside the staging results. If staging is 2x slower than production, something in the new code is causing a regression. The pipeline surfaces this data. The human decides whether it is acceptable.

---

## The Explain Summary

After staging validation passes, the pipeline generates a deployment summary. This is the "explain" beat from the DGVE pipeline applied to deployment.

```yaml title=".github/workflows/deploy-production.yml"
  explain-summary:
    name: Generate Deployment Summary
    runs-on: ubuntu-latest
    needs: [validate-staging]

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate deployment summary
        id: summary
        run: |
          # Get the merge commit and its parents
          MERGE_SHA=${{ github.sha }}
          PREV_DEPLOY=$(git tag -l 'deploy-*' --sort=-v:refname | head -1)

          if [ -z "$PREV_DEPLOY" ]; then
            PREV_DEPLOY=$(git rev-list --max-parents=0 HEAD)
          fi

          echo "## Deployment Summary" > deploy-summary.md
          echo "" >> deploy-summary.md
          echo "**Deploying:** \`${MERGE_SHA:0:7}\`" >> deploy-summary.md
          echo "**Previous deployment:** \`${PREV_DEPLOY}\`" >> deploy-summary.md
          echo "" >> deploy-summary.md

          # Changed files
          echo "### Files Changed" >> deploy-summary.md
          echo '```' >> deploy-summary.md
          git diff --stat $PREV_DEPLOY..$MERGE_SHA >> deploy-summary.md
          echo '```' >> deploy-summary.md
          echo "" >> deploy-summary.md

          # Commits since last deploy
          echo "### Commits" >> deploy-summary.md
          git log --oneline $PREV_DEPLOY..$MERGE_SHA >> deploy-summary.md
          echo "" >> deploy-summary.md

          # Infrastructure changes
          INFRA_CHANGES=$(git diff --name-only $PREV_DEPLOY..$MERGE_SHA -- 'infra/' '*.tf' | wc -l)
          if [ "$INFRA_CHANGES" -gt 0 ]; then
            echo "### Infrastructure Changes" >> deploy-summary.md
            echo "**$INFRA_CHANGES infrastructure files changed.**" >> deploy-summary.md
            echo "" >> deploy-summary.md
            git diff --name-only $PREV_DEPLOY..$MERGE_SHA -- 'infra/' '*.tf' >> deploy-summary.md
            echo "" >> deploy-summary.md
            echo "Review Terraform plan output before approving." >> deploy-summary.md
          else
            echo "### Infrastructure Changes" >> deploy-summary.md
            echo "No infrastructure files changed." >> deploy-summary.md
          fi
          echo "" >> deploy-summary.md

          # Risk assessment
          echo "### Risk Assessment" >> deploy-summary.md
          DB_CHANGES=$(git diff --name-only $PREV_DEPLOY..$MERGE_SHA -- '**/migrations/**' | wc -l)
          IAM_CHANGES=$(git diff $PREV_DEPLOY..$MERGE_SHA -- '*.tf' | grep -c 'aws_iam' || true)
          SG_CHANGES=$(git diff $PREV_DEPLOY..$MERGE_SHA -- '*.tf' | grep -c 'aws_security_group' || true)

          if [ "$DB_CHANGES" -gt 0 ]; then
            echo "- **Database migrations included** ($DB_CHANGES files)" >> deploy-summary.md
          fi
          if [ "$IAM_CHANGES" -gt 0 ]; then
            echo "- **IAM changes detected** ($IAM_CHANGES references)" >> deploy-summary.md
          fi
          if [ "$SG_CHANGES" -gt 0 ]; then
            echo "- **Security group changes detected** ($SG_CHANGES references)" >> deploy-summary.md
          fi
          if [ "$DB_CHANGES" -eq 0 ] && [ "$IAM_CHANGES" -eq 0 ] && [ "$SG_CHANGES" -eq 0 ]; then
            echo "- No high-risk changes detected (no DB migrations, IAM, or security groups)" >> deploy-summary.md
          fi
          echo "" >> deploy-summary.md

          # Staging validation results
          echo "### Staging Validation" >> deploy-summary.md
          echo "- Health check: Passed" >> deploy-summary.md
          echo "- Smoke tests: Passed" >> deploy-summary.md
          echo "- K6 load test: Passed (p95 < 500ms, error rate < 1%)" >> deploy-summary.md

      - name: Post summary to PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('deploy-summary.md', 'utf8');

            // Find the merged PR
            const { data: prs } = await github.rest.repos.listPullRequestsAssociatedWithCommit({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha
            });

            if (prs.length > 0) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prs[0].number,
                body: summary
              });
            }
```

The deployment summary answers four questions:

1. **What changed?** File diff stats and commit list since the last deployment.
2. **What was verified?** Staging validation results: health check, smoke tests, load test.
3. **What is risky?** Database migrations, IAM changes, security group modifications. These are the changes where mistakes are expensive.
4. **What is the blast radius?** Infrastructure file count and the specific categories of infrastructure touched.

This summary is what the human reviewer reads before clicking "approve." It takes 60 seconds to read. It replaces the 10-minute process of manually checking git logs, CI results, and staging health.

---

## The Human Approval Gate

This is the most important section in this post. Production deployment requires explicit human approval. No exceptions. No auto-approve for "small changes." No skip for "it only touches tests."

### Setting Up GitHub Environments

GitHub Environments provide built-in approval gates. Create a `production` environment with required reviewers.

:::steps
1. Go to your repository on GitHub
2. Navigate to **Settings > Environments**
3. Click **New environment**
4. Name it `production`
5. Check **Required reviewers**
6. Add yourself (and your co-founder, if applicable) as required reviewers
7. Optionally set **Wait timer** to 5 minutes (gives you time to read the deployment summary)
8. Click **Save protection rules**
:::

### Using the Environment in the Workflow

```yaml title=".github/workflows/deploy-production.yml"
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [explain-summary]
    environment:
      name: production
      url: https://shipfast.dev

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.PRODUCTION_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Deploy to production ECS
        run: |
          IMAGE_TAG=${{ github.sha }}

          # Register new task definition
          TASK_DEF_ARN=$(aws ecs register-task-definition \
            --cli-input-json file://ecs/task-definition-prod.json \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)

          echo "New task definition: $TASK_DEF_ARN"

          # Update service with new task definition
          aws ecs update-service \
            --cluster shipfast-production \
            --service shipfast-api \
            --task-definition $TASK_DEF_ARN \
            --force-new-deployment

          echo "Deployment initiated. Waiting for stability..."

      - name: Wait for production deployment
        run: |
          aws ecs wait services-stable \
            --cluster shipfast-production \
            --services shipfast-api
          echo "Production deployment stable"

      - name: Tag deployment
        run: |
          git tag "deploy-$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:7}"
          git push --tags
```

The `environment: production` block triggers the approval gate. When the workflow reaches this job, it pauses and sends a notification to the required reviewers. The reviewer sees the deployment summary (posted to the PR in the previous job) and either approves or rejects.

The deployment tag (`deploy-YYYYMMDD-HHMMSS-sha`) marks the exact commit that was deployed. The explain summary uses these tags to calculate the diff between deployments. Over time, these tags become your deployment history.

<Alert type="caution" title="Agent Trap">

Agents configure the deployment workflow without the `environment` block, creating a fully automated path from merge to production. The agent's reasoning: "CI passed, staging passed, all validations passed, so production deployment should be automatic." This reasoning ignores the entire class of issues that automated checks cannot catch: business context (is this a good time to deploy?), operational context (is there an ongoing incident?), and risk judgment (is a database migration during peak traffic acceptable?).

**What catches it:** Search the production workflow for `environment:`. If it is missing, the pipeline has no approval gate. Add it before your first production deployment, not after your first production incident.

</Alert>

---

## Post-Deploy Monitoring

After production deployment, the pipeline monitors key metrics for 15 minutes. This catches issues that appear under real traffic: memory leaks that build over minutes, connection pool exhaustion, cache miss storms after a cold start.

```yaml title=".github/workflows/deploy-production.yml"
  post-deploy-monitor:
    name: Post-Deploy Monitor (15 min)
    runs-on: ubuntu-latest
    needs: [deploy-production]

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.PRODUCTION_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Monitor for 15 minutes
        run: |
          echo "Monitoring production for 15 minutes..."
          echo "Start time: $(date -u)"

          CHECKS_PASSED=0
          CHECKS_TOTAL=15

          for i in $(seq 1 $CHECKS_TOTAL); do
            sleep 60

            # Health check
            HEALTH=$(curl -s -o /dev/null -w "%{http_code}" https://shipfast.dev/health)

            # Error rate from CloudWatch
            ERROR_RATE=$(aws cloudwatch get-metric-statistics \
              --namespace "ECS/Shipfast" \
              --metric-name "5xxErrorRate" \
              --dimensions Name=Environment,Value=production \
              --start-time $(date -u -d '2 minutes ago' +%Y-%m-%dT%H:%M:%S) \
              --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
              --period 60 \
              --statistics Average \
              --query 'Datapoints[-1].Average' \
              --output text 2>/dev/null || echo "0")

            # CPU utilization
            CPU=$(aws cloudwatch get-metric-statistics \
              --namespace "AWS/ECS" \
              --metric-name "CPUUtilization" \
              --dimensions Name=ClusterName,Value=shipfast-production Name=ServiceName,Value=shipfast-api \
              --start-time $(date -u -d '2 minutes ago' +%Y-%m-%dT%H:%M:%S) \
              --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
              --period 60 \
              --statistics Average \
              --query 'Datapoints[-1].Average' \
              --output text 2>/dev/null || echo "0")

            echo "Check $i/$CHECKS_TOTAL: Health=$HEALTH, 5xx=$ERROR_RATE%, CPU=$CPU%"

            if [ "$HEALTH" != "200" ]; then
              echo "ALERT: Health check failing. Investigate immediately."
              echo "Rollback command: aws ecs update-service --cluster shipfast-production --service shipfast-api --task-definition PREVIOUS_TASK_DEF"
              exit 1
            fi

            # Alert thresholds
            if (( $(echo "$ERROR_RATE > 5" | bc -l 2>/dev/null || echo 0) )); then
              echo "ALERT: Error rate above 5%. Current: $ERROR_RATE%"
              echo "Consider rollback."
            fi

            if (( $(echo "$CPU > 90" | bc -l 2>/dev/null || echo 0) )); then
              echo "WARNING: CPU utilization above 90%. Current: $CPU%"
            fi

            CHECKS_PASSED=$((CHECKS_PASSED + 1))
          done

          echo ""
          echo "Post-deploy monitoring complete."
          echo "Checks passed: $CHECKS_PASSED/$CHECKS_TOTAL"
          echo "No anomalies detected."
```

The monitoring checks three metrics every minute for 15 minutes:

| Metric | Alert Threshold | Action |
|--------|----------------|--------|
| Health check | Non-200 response | Fail pipeline, suggest rollback |
| 5xx error rate | Above 5% | Alert, suggest rollback |
| CPU utilization | Above 90% | Warning, monitor closely |

If the health check fails, the pipeline exits with a failure status and prints the rollback command. The pipeline does not execute the rollback automatically. That is a human decision.

---

## Rollback Strategy

Rollback is always a human decision. The pipeline provides the data and the commands. You make the call.

### When to Rollback

Rollback when any of these are true:

- Health check is failing and the cause is not immediately obvious
- 5xx error rate has sustained above 5% for more than 2 minutes
- A customer-facing feature is broken and there is no quick fix
- CPU or memory is climbing steadily (memory leak) with no plateau

Do not rollback when:

- A single transient 5xx occurred during the deployment transition (ECS rolling update briefly has old and new tasks)
- CPU spiked during deployment but stabilized within 2 minutes
- The issue is in a non-critical feature that can be fixed forward

### How to Rollback ECS

ECS makes rollback straightforward. Every deployment creates a new task definition revision. Rolling back means pointing the service at the previous revision.

```bash terminal
# List recent task definition revisions
aws ecs list-task-definitions \
  --family-prefix shipfast-api \
  --sort DESC \
  --max-items 5
```

<TerminalOutput title="Recent task definitions">

```
shipfast-api:42   (current - deployed just now)
shipfast-api:41   (previous - known good)
shipfast-api:40
shipfast-api:39
shipfast-api:38
```

</TerminalOutput>

```bash terminal
# Rollback to previous task definition
aws ecs update-service \
  --cluster shipfast-production \
  --service shipfast-api \
  --task-definition shipfast-api:41 \
  --force-new-deployment

# Wait for rollback to complete
aws ecs wait services-stable \
  --cluster shipfast-production \
  --services shipfast-api
```

Rollback takes 2-5 minutes depending on your deployment configuration (deregistration delay, health check grace period). During this time, both the broken revision and the rollback revision may be serving traffic.

### Infrastructure Rollback

If the deployment included Terraform changes, rolling back the application is not enough. You also need to consider whether the infrastructure change needs to be reverted.

Not all infrastructure changes are reversible. Deleting an S3 bucket is permanent. Changing an RDS instance class requires a restart. Modifying a VPC CIDR is irreversible.

For reversible infrastructure changes, revert the Terraform code in a new PR and apply:

```bash terminal
git revert HEAD
git push origin main
# This triggers a new deployment pipeline with the reverted code
```

For irreversible changes, you need a specific remediation plan. This is why the deployment summary flags infrastructure changes and why human approval exists.

---

## Scorecard Panels 22-25

This deployment pipeline generates data for four new Scorecard panels. These are the DORA metrics adapted for your agent-assisted workflow.

<ComparisonTable>
  <ComparisonHeader columns={["Panel", "Metric", "Data Source"]} />
  <ComparisonRow feature="22" Panel="PR-to-Production Lead Time" Metric="Time from PR merge to production deployment complete" Data_Source="GitHub API (merge timestamp), deploy tags" />
  <ComparisonRow feature="23" Panel="Deployment Frequency" Metric="Deployments per day/week" Data_Source="Deploy tags in git history" />
  <ComparisonRow feature="24" Panel="Rollback Rate" Metric="Percentage of deployments that required rollback" Data_Source="Git reverts, manual rollback commands" />
  <ComparisonRow feature="25" Panel="Post-Deploy Error Rate" Metric="5xx rate in the 15-minute monitoring window" Data_Source="CloudWatch metrics" />
</ComparisonTable>

These four panels join the existing 21 panels on your Scorecard. Together, they tell the story of your deployment health.

**Lead time** measures pipeline efficiency. If lead time is over 30 minutes, your pipeline has bottlenecks: slow tests, slow builds, or a long wait for approval. Target: under 20 minutes from merge to production.

**Deployment frequency** measures confidence. Teams that deploy frequently trust their pipeline. Teams that batch deployments into weekly releases do not. Target: at least once per day when changes are ready.

**Rollback rate** measures quality. A rollback rate above 10% means your verification steps are not catching issues. A rate below 2% means your pipeline is working. Target: under 5%.

**Post-deploy error rate** measures the gap between staging and production. If staging validation passes but production errors spike, your staging environment does not match production closely enough. Target: no increase in 5xx rate after deployment.

### Tracking Lead Time

Add this step to the post-deploy monitor job to record lead time:

```yaml title=".github/workflows/deploy-production.yml"
      - name: Record deployment metrics
        if: always()
        run: |
          DEPLOY_END=$(date +%s)

          # Get merge timestamp
          MERGE_TIME=$(git log -1 --format=%ct ${{ github.sha }})
          LEAD_TIME=$((DEPLOY_END - MERGE_TIME))

          echo "Deployment metrics:"
          echo "  Lead time: $((LEAD_TIME / 60)) minutes"
          echo "  Commit: ${{ github.sha }}"
          echo "  Status: ${{ job.status }}"

          # Log to a metrics file or send to your observability stack
          echo "$(date -u +%Y-%m-%dT%H:%M:%SZ),deploy,${{ github.sha }},$LEAD_TIME,${{ job.status }}" >> deploy-metrics.csv
```

---

## Complete Workflow Structure

<FileTree>
.github/
  workflows/
    ci.yml
    deploy-production.yml
  environments/
    production/
</FileTree>

The production workflow has six jobs:

| Job | Depends On | Duration | Automated |
|-----|-----------|----------|-----------|
| build-push | Trigger | 2-3 min | Yes |
| deploy-staging | build-push | 3-5 min | Yes |
| validate-staging | deploy-staging | 1-2 min | Yes |
| explain-summary | validate-staging | 30 sec | Yes |
| deploy-production | explain-summary | 3-5 min | Human approval required |
| post-deploy-monitor | deploy-production | 15 min | Yes |

**Total time from merge to monitored production: 25-30 minutes.** Of that, 15 minutes is the monitoring window. The deployment itself takes 10-15 minutes.

---

## Recalibration Checkpoint 3

This is the third recalibration checkpoint in the series. The first was in [Part 19](/blog/aws-for-startups/19-preview-environments) after building the scripted pipeline. The second was in [Part 34](/blog/aws-for-startups/34-k6-human-judgment) after performance testing. This one comes after completing your CI/CD pipeline.

Recalibration is not a retrospective. It is a data-driven adjustment of your agent workflow. Five steps, same procedure every time.

### Step 1: Run the Eval Framework

Run `eval-models.sh` from [Part 9](/blog/aws-for-startups/09-monorepo-context-evals) and compare results against your last run (from Recalibration 2 in Part 34).

```bash terminal
./scripts/eval/eval-models.sh --compare-to=recal-2
```

Questions to answer:

- **Are model scores improving, stable, or regressing?** If your primary model's score dropped more than 10%, investigate. Did the model update? Did you add new eval prompts that exposed a weakness?
- **Is the ranking still correct?** The best model in Part 34 may not be the best model now. Models update frequently. Run the eval to find out.
- **Are there new models to test?** If a new model has been released since your last recalibration, add it to the eval. The pipeline eval from [Part 43](/blog/aws-for-startups/43-full-stack-preview) tests model combinations (generator + verifier), not just individual models.

### Step 2: Review the Scorecard

You now have 25 Scorecard panels. Look at the data.

| Metric | Healthy | Investigate | Action |
|--------|---------|-------------|--------|
| Pre-commit pass rate | Above 90% | 70-90% | Below 70%: add rules to AGENT-INSTRUCTIONS.md |
| CI pass rate (agent PRs) | Above 85% | 70-85% | Below 70%: improve agent prompts or add pre-submit checks |
| Iterations-to-clean | 1-2 | 3 | 3+: improve prompts or switch generator model |
| PR-to-production lead time | Under 20 min | 20-30 min | Over 30 min: optimize slow pipeline steps |
| Rollback rate | Under 5% | 5-10% | Over 10%: add staging validation checks |
| Post-deploy error rate | No increase | Under 2% increase | Over 2%: staging environment does not match production |

If most metrics are in the "Healthy" column, your workflow is working. Do not change things that are working.

If metrics are in the "Investigate" column, dig deeper before making changes. A 75% CI pass rate might be caused by one flaky test, not by agent quality issues.

If metrics are in the "Action" column, make one change at a time and measure the impact over 5-10 deployments before making another change.

### Step 3: Review AGENT-INSTRUCTIONS.md

Your AGENT-INSTRUCTIONS.md has 71 lines covering IAM, Terraform, Git, Code Quality, Context Management, Secrets, Execution Security, Networking, API Design, Performance, Human Judgment, and Docker.

For each section, ask:

- **Is this rule being followed?** Check your Scorecard data. If a rule is consistently followed (>95% of the time), it is working. Keep it.
- **Is this rule being violated?** If a rule is violated more than 20% of the time, it is either unclear (rephrase it) or unenforceable at the instruction level (add a pre-commit hook or CI check).
- **Is this rule unnecessary?** If a rule was added as a reaction to a one-time issue and the issue has not recurred in 20+ generations, consider removing it. Fewer rules mean smaller prompts mean better context utilization.

### Step 4: Adjust Pipeline Intensity

The pipeline from [Part 43](/blog/aws-for-startups/43-full-stack-preview) has configurable iteration limits. Based on your data:

- **Iterations-to-clean consistently 1:** Your prompts and instructions are working well. Consider reducing the max iterations from 3 to 2 to save time.
- **Iterations-to-clean consistently 3+:** Your prompts need improvement, or the generator model is struggling with your codebase. Try a different generator model or add more context to the prompt.
- **Verification pass rate above 95% on first iteration:** You can consider running verification as a CI check instead of a pre-generation step, which is faster.

### Step 5: Update the Cost Budget

Check your API spend for the CI/CD phase:

- **Model API costs:** How much did you spend on generation, verification, and explanation calls?
- **CI compute costs:** How many GitHub Actions minutes did agent PRs consume vs human PRs?
- **AWS costs:** Did any agent-generated infrastructure increase your AWS bill unexpectedly?

The rule of thumb: if model API spend exceeds 5% of your infrastructure spend, optimize. Use cheaper models for verification (it works surprisingly well), reduce generation retries, or improve prompts so the first generation is cleaner.

### Recording Recalibration 3 Results

Create a recalibration record:

```markdown title="docs/recalibration/recal-3-cicd.md"
# Recalibration Checkpoint 3: CI/CD Phase

**Date:** YYYY-MM-DD
**Parts covered since last recalibration:** 35-47
**Deployments since last recalibration:** ___

## Eval Results
- Primary model score: ___ (was ___ at Recal 2)
- Change: +/- __%
- New model tested: ___ (score: ___)

## Scorecard Health
- Pre-commit pass rate: ___%
- CI pass rate (agent PRs): ___%
- Iterations-to-clean: ___
- PR-to-production lead time: ___ min
- Rollback rate: ___%
- Post-deploy error rate: ___%

## AGENT-INSTRUCTIONS.md Changes
- Rules added: ___
- Rules rephrased: ___
- Rules removed: ___
- Total lines: ___ (was 71)

## Pipeline Adjustments
- Max iterations: ___ (was ___)
- Models changed: ___

## Cost Review
- Model API spend: $___
- CI compute: $___
- Infra spend delta: $___
- API spend as % of infra: ___%

## Decision
[ ] No changes needed
[ ] Adjusted pipeline intensity
[ ] Changed model configuration
[ ] Updated AGENT-INSTRUCTIONS.md
[ ] Need deeper investigation (describe: ___)
```

---

## The Fine Line

<Alert type="warning" title="The Fine Line">

| | |
|---|---|
| âŒ **Under** | SSH into the production server and `git pull`. No staging. No approval. No monitoring. You find out the deployment is broken when a customer emails you, 4 hours after the fact. |
| âœ… **Right** | Automated pipeline with staging validation, explain summary, human approval gate via GitHub Environments, 15-minute post-deploy monitoring. Rollback commands printed but never auto-executed. 25-30 minutes from merge to monitored production. |
| âŒ **Over** | Blue-green deployment with 50/50 traffic splitting, canary analysis with automated rollback at 0.1% error rate, deployment windows with change advisory boards, and a 4-person approval chain. This is the right approach at 10,000 requests per second, not at 50. |
| ğŸ¤– **Agent Trap** | Agent configures the deployment workflow without the `environment:` block, creating a fully automated path from merge to production. The agent reasons that "all checks passed, so deployment is safe." The agent cannot evaluate business context (is this a good time to deploy?), operational context (are we mid-incident?), or risk judgment (should we deploy a database migration during peak traffic?). Human approval is the firewall between automated confidence and production reality. |

</Alert>

---

## What's Coming

Next in **Part 48: Lambda Fundamentals**, we step into serverless. Lambda functions for event-driven workloads, webhook handlers, and background jobs. You already have containers for your core API. Lambda handles the rest: the glue between services, the cron jobs, the image processors, the things that do not need a container running 24/7.

---

## Validation Checklist

<ValidationChecklist items={[
  {
    category: "Deployment Pipeline",
    tasks: [
      { text: "Production deployment workflow exists at .github/workflows/deploy-production.yml", syncKey: "part-47-workflow-exists" },
      { text: "Build job pushes images to ECR with git SHA tags", syncKey: "part-47-ecr-push" },
      { text: "Staging deployment runs before production", syncKey: "part-47-staging-first" },
      { text: "Staging validation includes health checks and K6 smoke test", syncKey: "part-47-staging-validation" }
    ]
  },
  {
    category: "Human Approval Gate",
    tasks: [
      { text: "GitHub Environment 'production' created with required reviewers", syncKey: "part-47-env-created" },
      { text: "deploy-production job uses environment: production", syncKey: "part-47-env-block" },
      { text: "Explain summary posts deployment details to the merged PR", syncKey: "part-47-explain-summary" }
    ]
  },
  {
    category: "Post-Deploy & Rollback",
    tasks: [
      { text: "Post-deploy monitoring runs for 15 minutes checking health, error rate, CPU", syncKey: "part-47-monitoring" },
      { text: "Deploy tags created for each production deployment", syncKey: "part-47-deploy-tags" },
      { text: "Rollback procedure documented and tested with ECS task definition rollback", syncKey: "part-47-rollback" }
    ]
  },
  {
    category: "Scorecard & Metrics",
    tasks: [
      { text: "Lead time metric recorded in deployment workflow", syncKey: "part-47-lead-time" },
      { text: "Four new Scorecard panels identified: lead time, frequency, rollback rate, error rate", syncKey: "part-47-scorecard-panels" }
    ]
  },
  {
    category: "Recalibration",
    tasks: [
      { text: "eval-models.sh run and compared to Recalibration 2 results", syncKey: "part-47-eval-run" },
      { text: "Scorecard metrics reviewed against healthy/investigate/action thresholds", syncKey: "part-47-scorecard-review" },
      { text: "AGENT-INSTRUCTIONS.md reviewed for rule effectiveness", syncKey: "part-47-instructions-review" },
      { text: "Pipeline intensity adjusted based on iterations-to-clean data", syncKey: "part-47-pipeline-adjust" },
      { text: "Recalibration 3 record created in docs/recalibration/", syncKey: "part-47-recal-recorded" }
    ]
  }
]} />

---

## Key Takeaways

1. Human approval for production deployment is non-negotiable: agents assist, surface data, and generate summaries, but humans decide when code reaches production.
2. The deployment summary (explain step) is the most underrated part of the pipeline: 60 seconds of reading replaces 10 minutes of manual investigation and catches risk categories that automated checks miss entirely.
3. Post-deploy monitoring for 15 minutes catches issues that staging cannot: memory leaks, connection pool exhaustion, and configuration mismatches between environments.
4. Rollback is always a human decision because the tradeoff between "broken feature" and "data loss from reverting a migration" requires judgment that no automated threshold can capture.
5. Recalibration checkpoints turn "I think the agent is getting better" into "the data shows the agent's CI pass rate improved from 78% to 91% after rephrasing three AGENT-INSTRUCTIONS.md rules," and that specificity is the difference between opinion and engineering.

---

*Have questions? Found an error? [Open an issue](https://github.com/hionnode/akasha-lekha/issues) or reach out on [LinkedIn](https://linkedin.com/in/chinmay-pandey).*
